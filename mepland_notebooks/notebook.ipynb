{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run code.py\n",
    "%matplotlib inline\n",
    "time_all_start = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file and tree names\n",
    "\n",
    "# MC\n",
    "sig_file_name = 'data/all_el.root'\n",
    "bkg_file_name = 'data/all_mu.root'\n",
    "sig_tree  = 'electron_mc'\n",
    "bkg_tree  = 'muon_mc'\n",
    "\n",
    "# data\n",
    "# sig_tree  = 'electron_tags'\n",
    "# bkg_tree  = 'muons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other settings\n",
    "\n",
    "fit_verbose = 1\n",
    "\n",
    "# max_epochs = 100\n",
    "max_epochs = 20\n",
    "max_epochs_model_default = 50\n",
    "\n",
    "default_to_load = True\n",
    "\n",
    "rnd_seed = 7\n",
    "\n",
    "plot_mi = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup variables to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_variables = OrderedDict([\n",
    "    ('p',['$p$','default']),\n",
    "    ('pT',['$p_{\\mathrm{T}}$','default']),\n",
    "    ('eta',['$\\eta$','symmetric']),\n",
    "    ('nTRThitsMan',['nTRT','default']),\n",
    "    ('nTRTouts',['nTRT outs','default']),\n",
    "    ('fHTMB',['Fraction HTMB','default']),\n",
    "    ('fAr',['Fraction Ar','default']),\n",
    "    ('trkOcc',['Track Occ.','default']),\n",
    "    ('sumToTsumL',['$\\sum\\mathrm{ToT} / \\sum L$','default']),\n",
    "    ('PHF',['PHF','default']),\n",
    "    ('sumL',['sumL','default']),\n",
    "    ('eProbHT',['eProbHT','default']),\n",
    "#    ('NhitsdEdx',['NhitsdEdx','default']), # TODO invalid values\n",
    "#    ('phi',['$\\phi$','default']), # Didn't add anything, don't expect it to physically though\n",
    "])\n",
    "\n",
    "# all the hit vars, arrays of length 40 for use in RNN LSTM\n",
    "\n",
    "###########\n",
    "# uninteresting vars - info is already included elsewhere\n",
    "# nTRThits, nArhits, nXehits, nHThitsMan, nPrechitsMan, sumToT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names_dict = {k:v[0] for (k,v) in input_variables.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_comb_dir = ''\n",
    "for i,v in enumerate(input_variables.keys()):\n",
    "    if i != 0: var_comb_dir += '_'\n",
    "    var_comb_dir += v\n",
    "plots_path = 'plots/'+var_comb_dir\n",
    "models_path = 'models/'+var_comb_dir\n",
    "make_path(plots_path)\n",
    "make_path(models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sig, df_bkg, X_train, X_test, y_train, y_test = create_df_tts_scale(\n",
    "#    sig_file_name, sig_tree, bkg_file_name, bkg_tree,\n",
    "#    list(input_variables),\n",
    "#    test_size=0.2,\n",
    "#    # test_size=0.333333,\n",
    "#    # sig_n=50000,\n",
    "#    # bkg_n=50000,\n",
    "#    shuffle=True,\n",
    "#    scale_style={i:v[1] for i,(_,v) in enumerate(input_variables.items())}\n",
    "# )\n",
    "\n",
    "# TODO WARNING df_'s are not weighted!\n",
    "\n",
    "df_sig, df_bkg, X_train, X_test, y_train, y_test = create_fixed_test_shuffled_train_and_scale(\n",
    "    sig_file_name, sig_tree, bkg_file_name, bkg_tree,\n",
    "    list(input_variables),\n",
    "    test_size=0.2,\n",
    "    # test_size=0.333333,\n",
    "    # sig_n=50000,\n",
    "    # bkg_n=50000,\n",
    "    scale_style={i:v[1] for i,(_,v) in enumerate(input_variables.items())},\n",
    "    rnd_seed = rnd_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ndimensions = X_train.shape[1]\n",
    "leptons_m = y_train.shape[0]\n",
    "\n",
    "print(\"Training on m = %.2g leptons\\nTesting on %.2g leptons (50/50 sig/bkg)\\nNumber of input variables n = %d\" % (leptons_m, y_test.shape[0], input_ndimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(df_sig.head(3))\n",
    "    print(X_train.shape)\n",
    "    print(X_train[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find weights to normalize pT, eta distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_train, w_test = weight_pT_eta_uniform(input_variables, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data=(X_test, y_test, w_test) # Always use the same test data for non-kfold runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pT_eta(input_variables, X_train, y_train, plots_path, name='$p_{\\mathrm{T}}$ vs $\\eta$', fname='pT_eta_hist', w = None)\n",
    "plot_pT_eta(input_variables, X_train, y_train, plots_path, name='$p_{\\mathrm{T}}$ vs $\\eta$, Weighted', fname='pT_eta_hist_weighted', w = w_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create eProbabilityHT curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO this technically has the issue with the high pT events, but shouldn't actually be visible on plots\n",
    "sig_eprob = uproot.open(sig_file_name)[sig_tree].array('eProbHT')\n",
    "bkg_eprob = uproot.open(bkg_file_name)[bkg_tree].array('eProbHT')\n",
    "\n",
    "m_eprob = min(sig_eprob.shape[0], bkg_eprob.shape[0])\n",
    "sig_eprob = sig_eprob[:m_eprob]\n",
    "bkg_eprob = bkg_eprob[:m_eprob]\n",
    "print('Using %.2g sig el, %.2g bkg for eProbHT' % (sig_eprob.shape[0], bkg_eprob.shape[0]))\n",
    "\n",
    "roc_eprob_obj = eprob_roc_generateor(sig_eprob, bkg_eprob)\n",
    "\n",
    "roc_eprob = [roc_eprob_obj.tpr(), roc_eprob_obj.fpr(), 'eProbHT', 'eprob', 'black', '-']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_input_vars(input_variables, X_train, y_train, plots_path, 'Unweighted')\n",
    "plot_all_input_vars(input_variables, X_train, y_train, plots_path, 'Weighted', 'all_input_vars_weighted', False, w_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_scale_example(sig_file_name,sig_tree,plots_path,'p','$p$ [GeV]'\n",
    "# plot_scale_example(sig_file_name,sig_tree,plots_path,'pT','$p_{\\mathrm{T}}$ [GeV]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDT (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt_m = min(100000, y_train.shape[0])\n",
    "# bdt_m = None # all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_bdt1 = 'bdt1'\n",
    "train_load_bdt1 = train_or_load(models_path+'/'+fname_bdt1+'.pkl', default_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_load_bdt1 == 'n':\n",
    "    \n",
    "    # create   \n",
    "    bdt1 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2),\n",
    "                              algorithm=\"SAMME\",\n",
    "                              n_estimators=200,\n",
    "                             )\n",
    "    \n",
    "    # train\n",
    "    train_start = datetime.now()\n",
    "\n",
    "    # bdt1.fit(X_train,y_train, sample_weight=w_train);\n",
    "    bdt1.fit(X_train[:bdt_m],y_train[:bdt_m], sample_weight=w_train[:bdt_m]);\n",
    "\n",
    "    print(strfdelta(datetime.now()-train_start, \"Training time: {hours} hours, {minutes} minutes, {seconds} seconds\"))\n",
    "\n",
    "    # save model to pickle\n",
    "    joblib.dump(bdt1, models_path+'/'+fname_bdt1+'.pkl');\n",
    "    \n",
    "else:\n",
    "    # load model from pickle\n",
    "    bdt1 = joblib.load(models_path+'/'+fname_bdt1+'.pkl');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier_1D_output(bdt1.decision_function(X_test[y_test>0.5]), # sig\n",
    "                          bdt1.decision_function(X_test[y_test<0.5]), # bkg\n",
    "                          'BDT', 'bdt', plots_path\n",
    "                         )\n",
    "\n",
    "fpr_bdt1, tpr_bdt1, thresholds_bdt1 = roc_curve(y_test, bdt1.decision_function(X_test))\n",
    "roc_bdt1 = [tpr_bdt1, fpr_bdt1, 'BDT', 'bdt', 'green', '-.']\n",
    "\n",
    "plot_roc([roc_eprob, roc_bdt1], plots_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_m = min(50000, y_train.shape[0])\n",
    "# goes as n*svm_m*log(svm_m)\n",
    "# ~1 hour, 10 minutes for 100000\n",
    "# ~TODO minutes for 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_svm1 = 'svm1'\n",
    "train_load_svm1 = train_or_load(models_path+'/'+fname_svm1+'.pkl', default_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_load_svm1 == 'n':\n",
    "    \n",
    "    # create\n",
    "    svm1 = svm.SVC(#C=1.0, #kernel='rbf', #tol=0.001, #gamma='auto',\n",
    "    probability=True,\n",
    "    cache_size=1000, # default is 200 (MB)\n",
    "    verbose=False);\n",
    "\n",
    "    # train\n",
    "    train_start = datetime.now()\n",
    "\n",
    "    svm1.fit(X_train[:svm_m],y_train[:svm_m], sample_weight=w_train[:svm_m]);\n",
    "\n",
    "    print(strfdelta(datetime.now()-train_start, \"Training time: {hours} hours, {minutes} minutes, {seconds} seconds\"))\n",
    "\n",
    "    # save model to pickle\n",
    "    joblib.dump(svm1, models_path+'/'+fname_svm1+'.pkl');\n",
    "    \n",
    "else:\n",
    "    # load model from pickle\n",
    "    svm1 = joblib.load(models_path+'/'+fname_svm1+'.pkl');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier_1D_output(svm1.decision_function(X_test[y_test>0.5]), # sig\n",
    "                          svm1.decision_function(X_test[y_test<0.5]), # bkg\n",
    "                          'SVM', 'svm', plots_path\n",
    "                         )\n",
    "\n",
    "svm1_roc_start = datetime.now()\n",
    "fpr_svm1, tpr_svm1, thresholds_svm1 = roc_curve(y_test, svm1.decision_function(X_test))\n",
    "roc_svm1 = [tpr_svm1, fpr_svm1, 'SVM', 'svm', 'blue', ':']\n",
    "print(strfdelta(datetime.now()-svm1_roc_start, \"SVM ROC production time: {hours} hours, {minutes} minutes, {seconds} seconds\"))\n",
    "\n",
    "plot_roc([roc_eprob, roc_svm1], plots_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras / Tensorflow Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(rnd_seed)\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    from keras.models import load_model\n",
    "    from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_model_default = 'model_default'\n",
    "train_load_model_default = train_or_load(models_path+'/'+fname_model_default+'.h5', default_to_load)\n",
    "train_load_model_default = 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_load_model_default == 'n':\n",
    "    \n",
    "    # create\n",
    "    model_default = Sequential()\n",
    "    model_default.add(Dense(12, input_dim=input_ndimensions, activation='relu'))\n",
    "    model_default.add(Dense(8, activation='relu'))\n",
    "    model_default.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model_default.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model_default_callbacks = [] \n",
    "    model_default_callbacks.append(EarlyStopping(\n",
    "        monitor='acc', min_delta=0.0002,\n",
    "        # monitor='loss', min_delta=0.00002,                                                 \n",
    "        patience=5,\n",
    "        verbose=0,\n",
    "        mode='auto'))\n",
    "    \n",
    "    # train\n",
    "    train_start = datetime.now()\n",
    "    hist_model_default = model_default.fit(X_train, y_train,\n",
    "                                           epochs=max_epochs_model_default, batch_size=50,\n",
    "                                           verbose=fit_verbose, validation_data=val_data,\n",
    "                                           sample_weight=w_train,\n",
    "                                           callbacks=model_default_callbacks);\n",
    "\n",
    "    hist_dict_model_default = hist_model_default.history\n",
    "    print(strfdelta(datetime.now()-train_start, \"Training time: {hours} hours, {minutes} minutes, {seconds} seconds\"))\n",
    "\n",
    "    # save model to HDF5, history to pickle\n",
    "    model_default.save(models_path+'/'+fname_model_default+'.h5')\n",
    "   \n",
    "    with open(models_path+'/'+fname_model_default+'_hist.pickle', 'wb') as handle:\n",
    "        pickle.dump(hist_dict_model_default, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "else:\n",
    "    # load model from HDF5, history from pickle\n",
    "    model_default = load_model(models_path+'/'+fname_model_default+'.h5')\n",
    "    \n",
    "    with open(models_path+'/'+fname_model_default+'_hist.pickle', 'rb') as handle:\n",
    "        hist_dict_model_default = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss_vs_epoch(hist_dict_model_default, 'NN (Default)', 'nn_default', plots_path, 'Test', True, False)\n",
    "plot_acc_loss_vs_epoch(hist_dict_model_default, 'NN (Default)', 'nn_default', plots_path, 'Test', False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model_default %s: %.2f%%\" % (model_default.metrics_names[1], model_default.evaluate(X_test,y_test,verbose=0)[1]*100))\n",
    "\n",
    "plot_classifier_1D_output(model_default.predict(X_test[y_test>0.5], verbose=0), # sig\n",
    "                          model_default.predict(X_test[y_test<0.5], verbose=0), # bkg\n",
    "                          'NN (Default)', 'nn_default', plots_path\n",
    "                         )\n",
    "\n",
    "fpr_model_default, tpr_model_default, thresholds_model_default = roc_curve(y_test, model_default.predict(X_test, verbose=0))\n",
    "roc_model_default = [tpr_model_default, fpr_model_default, 'NN (Default)', 'nn_default', 'magenta', '--']\n",
    "\n",
    "plot_roc([roc_eprob, roc_model_default], plots_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print and plot model_default structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_default.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "# pip install pydot\n",
    "\n",
    "plot_model(model_default, to_file=plots_path+'/model_default.pdf', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print input variable plots vs default NN ouptut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_variables_with_model_default_output = input_variables.copy() \n",
    "input_variables_with_model_default_output['model_default_nn_output'] = ['NN (Default) Output', 'leave']\n",
    "model_default_output_bins = [0.0, 0.05, 0.1, 0.15, 0.2,\n",
    "                             0.4, 0.6,\n",
    "                             0.8, 0.85, 0.9, 0.95, 1.0]\n",
    "\n",
    "X_train_with_model_default_output = np.append(X_train, model_default.predict(X_train, verbose=0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_and_plot_all_input_vars('model_default_nn_output', 'NN', model_default_output_bins, \n",
    "                              input_variables_with_model_default_output,\n",
    "                              X_train_with_model_default_output,\n",
    "                              y_train, plots_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout (Larger network, higher learning rate, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_model_dropout = 'model_dropout'\n",
    "train_load_model_dropout = train_or_load(models_path+'/'+fname_model_dropout+'.h5', default_to_load)\n",
    "train_load_model_dropout = 'n' # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_load_model_dropout == 'n':\n",
    "    \n",
    "    # create\n",
    "    model_dropout = Sequential()\n",
    "    model_dropout.add(Dense(12, input_dim=input_ndimensions, activation='relu'))\n",
    "    model_dropout.add(Dense(8, activation='relu'))\n",
    "    model_dropout.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model_dropout.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model_dropout_callbacks = [] \n",
    "    \n",
    "    model_dropout_callbacks.append(EarlyStopping(\n",
    "        monitor='acc', min_delta=0.0002,\n",
    "        # monitor='loss', min_delta=0.00002,                                                 \n",
    "        patience=5,\n",
    "        verbose=0,\n",
    "        mode='auto'))\n",
    "    \n",
    "    # train\n",
    "    train_start = datetime.now()\n",
    "    hist_model_dropout = model_dropout.fit(X_train, y_train,\n",
    "                                           epochs=max_epochs_model_dropout, batch_size=50,\n",
    "                                           verbose=fit_verbose, validation_data=val_data,\n",
    "                                           sample_weight=w_train,\n",
    "                                           callbacks=model_dropout_callbacks);\n",
    "\n",
    "    hist_dict_model_dropout = hist_model_dropout.history\n",
    "    print(strfdelta(datetime.now()-train_start, \"Training time: {hours} hours, {minutes} minutes, {seconds} seconds\"))\n",
    "\n",
    "    # save model to HDF5, history to pickle\n",
    "    model_dropout.save(models_path+'/'+fname_model_dropout+'.h5')\n",
    "   \n",
    "    with open(models_path+'/'+fname_model_dropout+'_hist.pickle', 'wb') as handle:\n",
    "        pickle.dump(hist_dict_model_dropout, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "else:\n",
    "    # load model from HDF5, history from pickle\n",
    "    model_dropout = load_model(models_path+'/'+fname_model_dropout+'.h5')\n",
    "    \n",
    "    with open(models_path+'/'+fname_model_dropout+'_hist.pickle', 'rb') as handle:\n",
    "        hist_dict_model_dropout = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss_vs_epoch(hist_dict_model_dropout, 'NN (dropout)', 'nn_dropout', plots_path, 'Test', True, False)\n",
    "plot_acc_loss_vs_epoch(hist_dict_model_dropout, 'NN (dropout)', 'nn_dropout', plots_path, 'Test', False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model_dropout %s: %.2f%%\" % (model_dropout.metrics_names[1], model_dropout.evaluate(X_test,y_test,verbose=0)[1]*100))\n",
    "\n",
    "plot_classifier_1D_output(model_dropout.predict(X_test[y_test>0.5], verbose=0), # sig\n",
    "                          model_dropout.predict(X_test[y_test<0.5], verbose=0), # bkg\n",
    "                          'NN (dropout)', 'nn_dropout', plots_path\n",
    "                         )\n",
    "\n",
    "fpr_model_dropout, tpr_model_dropout, thresholds_model_dropout = roc_curve(y_test, model_dropout.predict(X_test, verbose=0))\n",
    "roc_model_dropout = [tpr_model_dropout, fpr_model_dropout, 'NN (dropout)', 'nn_dropout', 'maroon', '-.']\n",
    "\n",
    "plot_roc([roc_eprob, roc_model_dropout], plots_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 - TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout and L2? - TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_model_wide = 'model_wide'\n",
    "train_load_model_wide = train_or_load(models_path+'/'+fname_model_wide+'.h5', default_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_load_model_wide == 'n':\n",
    "    \n",
    "    # create\n",
    "    model_wide = Sequential()\n",
    "    model_wide.add(Dense(24, input_dim=input_ndimensions, activation='relu'))\n",
    "    model_wide.add(Dense(16, activation='relu'))\n",
    "    model_wide.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model_wide.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # train\n",
    "    train_start = datetime.now()\n",
    "    hist_model_wide = model_wide.fit(X_train, y_train,\n",
    "                                     epochs=max_epochs, batch_size=50,\n",
    "                                     verbose=fit_verbose, validation_data=val_data,\n",
    "                                     sample_weight=w_train);\n",
    "\n",
    "    hist_dict_model_wide = hist_model_wide.history\n",
    "    print(strfdelta(datetime.now()-train_start, \"Training time: {hours} hours, {minutes} minutes, {seconds} seconds\"))\n",
    "\n",
    "    # save model to HDF5, history to pickle\n",
    "    model_wide.save(models_path+'/'+fname_model_wide+'.h5')\n",
    "   \n",
    "    with open(models_path+'/'+fname_model_wide+'_hist.pickle', 'wb') as handle:\n",
    "        pickle.dump(hist_dict_model_wide, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "else:\n",
    "    # load model from HDF5, history from pickle\n",
    "    model_wide = load_model(models_path+'/'+fname_model_wide+'.h5')\n",
    "    \n",
    "    with open(models_path+'/'+fname_model_wide+'_hist.pickle', 'rb') as handle:\n",
    "        hist_dict_model_wide = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss_vs_epoch(hist_dict_model_wide, 'NN (wide)', 'nn_wide', plots_path, 'Test', True, False)\n",
    "plot_acc_loss_vs_epoch(hist_dict_model_wide, 'NN (wide)', 'nn_wide', plots_path, 'Test', False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model_wide %s: %.2f%%\" % (model_wide.metrics_names[1], model_wide.evaluate(X_test,y_test,verbose=0)[1]*100))\n",
    "\n",
    "plot_classifier_1D_output(model_wide.predict(X_test[y_test>0.5], verbose=0), # sig\n",
    "                          model_wide.predict(X_test[y_test<0.5], verbose=0), # bkg\n",
    "                          'NN (wide)', 'nn_wide', plots_path\n",
    "                         )\n",
    "\n",
    "fpr_model_wide, tpr_model_wide, thresholds_model_wide = roc_curve(y_test, model_wide.predict(X_test, verbose=0))\n",
    "roc_model_wide = [tpr_model_wide, fpr_model_wide, 'NN (wide)', 'nn_wide', 'cyan', '-.']\n",
    "\n",
    "plot_roc([roc_eprob, roc_model_wide], plots_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_model_deep = 'model_deep'\n",
    "train_load_model_deep = train_or_load(models_path+'/'+fname_model_deep+'.h5', default_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_load_model_deep == 'n':\n",
    "    \n",
    "    # create\n",
    "    model_deep = Sequential()\n",
    "    model_deep.add(Dense(12, input_dim=input_ndimensions, activation='relu'))\n",
    "    model_deep.add(Dense(8, activation='relu'))\n",
    "    model_deep.add(Dense(8, activation='relu'))\n",
    "    model_deep.add(Dense(8, activation='relu'))\n",
    "    model_deep.add(Dense(8, activation='relu'))\n",
    "    model_deep.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model_deep.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # train\n",
    "    train_start = datetime.now()\n",
    "    hist_model_deep = model_deep.fit(X_train, y_train,\n",
    "                                     epochs=max_epochs, batch_size=50,\n",
    "                                     verbose=fit_verbose, validation_data=val_data,\n",
    "                                     sample_weight=w_train);\n",
    "\n",
    "    hist_dict_model_deep = hist_model_deep.history\n",
    "    print(strfdelta(datetime.now()-train_start, \"Training time: {hours} hours, {minutes} minutes, {seconds} seconds\"))\n",
    "\n",
    "    # save model to HDF5, history to pickle\n",
    "    model_deep.save(models_path+'/'+fname_model_deep+'.h5')\n",
    "   \n",
    "    with open(models_path+'/'+fname_model_deep+'_hist.pickle', 'wb') as handle:\n",
    "        pickle.dump(hist_dict_model_deep, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "else:\n",
    "    # load model from HDF5, history from pickle\n",
    "    model_deep = load_model(models_path+'/'+fname_model_deep+'.h5')\n",
    "    \n",
    "    with open(models_path+'/'+fname_model_deep+'_hist.pickle', 'rb') as handle:\n",
    "        hist_dict_model_deep = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss_vs_epoch(hist_dict_model_deep, 'NN (deep)', 'nn_deep', plots_path, 'Test', True, False)\n",
    "plot_acc_loss_vs_epoch(hist_dict_model_deep, 'NN (deep)', 'nn_deep', plots_path, 'Test', False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"model_deep %s: %.2f%%\" % (model_deep.metrics_names[1], model_deep.evaluate(X_test,y_test,verbose=0)[1]*100))\n",
    "\n",
    "plot_classifier_1D_output(model_deep.predict(X_test[y_test>0.5], verbose=0), # sig\n",
    "                          model_deep.predict(X_test[y_test<0.5], verbose=0), # bkg\n",
    "                          'NN (deep)', 'nn_deep', plots_path\n",
    "                         )\n",
    "\n",
    "fpr_model_deep, tpr_model_deep, thresholds_model_deep = roc_curve(y_test, model_deep.predict(X_test, verbose=0))\n",
    "roc_model_deep = [tpr_model_deep, fpr_model_deep, 'NN (deep)', 'nn_deep', 'darkorange', '--']\n",
    "\n",
    "plot_roc([roc_eprob, roc_model_deep], plots_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = []\n",
    "all_models.append(roc_eprob)\n",
    "all_models.append(roc_bdt1)\n",
    "all_models.append(roc_svm1)\n",
    "all_models.append(roc_model_default)\n",
    "all_models.append(roc_model_wide)\n",
    "all_models.append(roc_model_deep)\n",
    "\n",
    "plot_roc(all_models, plots_path)\n",
    "\n",
    "roc_model_default_clean = list(roc_model_default)\n",
    "roc_model_default_clean[2] = 'NN'\n",
    "roc_model_default_clean[3] += '_clean'\n",
    "\n",
    "plot_roc([roc_eprob, roc_bdt1, roc_svm1], plots_path)\n",
    "plot_roc([roc_eprob, roc_bdt1, roc_model_default_clean], plots_path)\n",
    "plot_roc([roc_eprob, roc_svm1, roc_model_default_clean], plots_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-Fold of Default NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold_splits = 5\n",
    "kfold_max_epochs = 40\n",
    "kfold_fit_verbose = 0\n",
    "\n",
    "kfold_default_to_load = True\n",
    "\n",
    "plots_path_kfold = plots_path+'/kfold'\n",
    "models_path_kfold = models_path+'/kfold'\n",
    "make_path(plots_path_kfold)\n",
    "make_path(models_path_kfold)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=kfold_splits, shuffle=True, random_state=rnd_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs=[]\n",
    "losses=[]\n",
    "val_accs=[]\n",
    "val_losses=[]\n",
    "\n",
    "for fold_index, (train_indices, val_indices) in enumerate(skf.split(X_train, y_train)):\n",
    "    \n",
    "    fname_mode_kfold = (\"fold_%d\" % fold_index)\n",
    "    train_load_model_kfold = train_or_load(models_path_kfold+'/'+fname_mode_kfold+'.h5', kfold_default_to_load)\n",
    "\n",
    "    if train_load_model_kfold == 'n':\n",
    "\n",
    "        print(\"Training on fold {0:d}/{1:d}\".format(fold_index+1, kfold_splits))\n",
    "        fold_start = datetime.now()\n",
    "    \n",
    "        # Generate batches from indices\n",
    "        this_X_train, this_X_val = X_train[train_indices], X_train[val_indices]\n",
    "        this_y_train, this_y_val = y_train[train_indices], y_train[val_indices]\n",
    "        this_w_train, this_w_val = w_train[train_indices], w_train[val_indices]\n",
    "\n",
    "        this_w_train = w_train[train_indices]\n",
    "    \n",
    "        # create\n",
    "        model_kfold = Sequential()\n",
    "        model_kfold.add(Dense(12, input_dim=input_ndimensions, activation='relu'))\n",
    "        model_kfold.add(Dense(8, activation='relu'))\n",
    "        model_kfold.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model_kfold.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        # model_kfold_callbacks = [] \n",
    "        # model_kfold_callbacks.append(EarlyStopping(\n",
    "        #     monitor='acc', min_delta=0.0002,\n",
    "        #     monitor='loss', min_delta=0.00002,                                                 \n",
    "        #     patience=5,\n",
    "        #     verbose=0,\n",
    "        #     mode='auto'))\n",
    "\n",
    "        # train\n",
    "        train_start = datetime.now()\n",
    "        hist_model_kfold = model_kfold.fit(this_X_train, this_y_train,\n",
    "                                           epochs=kfold_max_epochs, batch_size=50,\n",
    "                                           verbose=kfold_fit_verbose, validation_data=(this_X_val, this_y_val, this_w_val),\n",
    "                                           sample_weight=this_w_train,\n",
    "                                          # callbacks=model_kfold_callbacks\n",
    "                                          );\n",
    "    \n",
    "        print(strfdelta(datetime.now()-fold_start, \"Training time: {hours} hours, {minutes} minutes, {seconds} seconds\"))\n",
    "\n",
    "        hist_dict_model_kfold = hist_model_kfold.history\n",
    "        \n",
    "        # save model to HDF5, history to pickle\n",
    "        model_kfold.save(models_path_kfold+'/'+fname_mode_kfold+'.h5')\n",
    "   \n",
    "        with open(models_path_kfold+'/'+fname_mode_kfold+'_hist.pickle', 'wb') as handle:\n",
    "            pickle.dump(hist_dict_model_kfold, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    else:\n",
    "        # load model from HDF5, history from pickle\n",
    "        model_kfold = load_model(models_path_kfold+'/'+fname_mode_kfold+'.h5')\n",
    "    \n",
    "        with open(models_path_kfold+'/'+fname_mode_kfold+'_hist.pickle', 'rb') as handle:\n",
    "            hist_dict_model_kfold = pickle.load(handle)\n",
    "        \n",
    "    # save hist to kfold lists, make plots\n",
    "    accs.append(hist_dict_model_kfold['acc'])\n",
    "    losses.append(hist_dict_model_kfold['loss'])\n",
    "    val_accs.append(hist_dict_model_kfold['val_acc'])\n",
    "    val_losses.append(hist_dict_model_kfold['val_loss'])\n",
    "\n",
    "    kfold_name = 'NN (fold {:d}/{:d})'.format(fold_index+1, kfold_splits)\n",
    "    kfold_nname = 'nn_fold_{:d}'.format(fold_index+1)\n",
    "\n",
    "    plot_acc_loss_vs_epoch(hist_dict_model_kfold, kfold_name, kfold_nname, plots_path_kfold, 'Validation', True, False)\n",
    "    plot_acc_loss_vs_epoch(hist_dict_model_kfold, kfold_name, kfold_nname, plots_path_kfold, 'Validation', False, True)\n",
    " \n",
    "    print(\"On test (not valid) data, this kfold %s: %.2f%%\" % (model_kfold.metrics_names[1], model_kfold.evaluate(X_test,y_test,verbose=0)[1]*100))\n",
    "\n",
    "    plot_classifier_1D_output(model_kfold.predict(X_test[y_test>0.5], verbose=0), # sig\n",
    "                              model_kfold.predict(X_test[y_test<0.5], verbose=0), # bkg\n",
    "                              kfold_name, kfold_nname, plots_path_kfold)\n",
    "\n",
    "    fpr_model_kfold, tpr_model_kfold, thresholds_model_kfold = roc_curve(y_test, model_kfold.predict(X_test, verbose=0))\n",
    "    roc_model_kfold = [tpr_model_kfold, fpr_model_kfold, kfold_name, kfold_nname, 'magenta', '--']\n",
    "\n",
    "    plot_roc([roc_eprob, roc_model_kfold], plots_path_kfold)\n",
    "\n",
    "    print(strfdelta(datetime.now()-fold_start, \"\\nTotal fold time: {hours} hours, {minutes} minutes, {seconds} seconds\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_kfold_hist_elements(accs, losses, val_accs, val_losses, plots_path,\n",
    "                            'NN ({:d}-fold)'.format(kfold_splits),\n",
    "                            'nn_{:d}fold'.format(kfold_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_mi: mutual_info_plot(var_names_dict, df_sig, 'Training Vars: Signal ($e$)', 'train_var_sig', plots_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_mi: mutual_info_plot(var_names_dict, df_bkg, 'Training Vars: Background', 'train_var_bkg', plots_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_mi: mutual_info_plot(var_names_dict,\n",
    "                             pd.concat([df_sig, df_bkg]),\n",
    "                             'Training Vars: Signal ($e$) & Background', 'train_var_sig_bkg', plots_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars=[\n",
    "'p',\n",
    "'pT',\n",
    "'eta',\n",
    "'nTRThitsMan',\n",
    "'nTRTouts',\n",
    "'fHTMB',\n",
    "'fAr',\n",
    "'trkOcc',\n",
    "'sumToTsumL',\n",
    "# 'lep_pT',\n",
    "'phi',\n",
    "'PHF',\n",
    "# 'NhitsdEdx',\n",
    "'sumToT',\n",
    "'sumL',\n",
    "'nTRThits',\n",
    "'nArhits',\n",
    "'nXehits',\n",
    "'nHThitsMan',\n",
    "'nPrechitsMan',\n",
    "'eProbHT'\n",
    "]\n",
    "\n",
    "if plot_mi:\n",
    "    df_sig_all_vars = create_df(sig_file_name, sig_tree, all_vars)\n",
    "    df_bkg_all_vars = create_df(bkg_file_name, bkg_tree, all_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_mi: mutual_info_plot({var:var for var in all_vars}, df_sig_all_vars, 'All Vars: Signal ($e$)', 'all_var_sig', 'plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_mi: mutual_info_plot({var:var for var in all_vars}, df_bkg_all_vars, 'All Vars: Background', 'all_var_bkg', 'plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_mi: mutual_info_plot({var:var for var in all_vars},\n",
    "                             pd.concat([df_sig_all_vars, df_bkg_all_vars]),\n",
    "                             'All Vars: Signal ($e$) & Background', 'all_var_sig_bkg', 'plots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total elapsed time: %s\" % (strfdelta(datetime.now()-time_all_start, \"{hours} hours, {minutes} minutes, {seconds} seconds\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Reference ReLU and Sigmoid diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-10,10,100)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "y = np.array([max(xi,0) for xi in x])\n",
    "ax.plot(x, y, lw=2, c='black', ls='-', label='ReLU')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_xticks([-10,-5,0,5,10])\n",
    "plt.figtext(0.3, 0.8, '$R(x) = \\max(0,x)$', ha='center', va='center', size=16)\n",
    "plt.title('ReLU');\n",
    "fig.savefig('plots/relu.pdf')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "y = np.array([1./(1.+np.exp(-xi)) for xi in x])\n",
    "ax.plot(x, y, lw=2, c='black', ls='-', label='Sigmoid')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_xticks([-10,-5,0,5,10])\n",
    "ax.set_yticks([0,1])\n",
    "plt.figtext(0.3, 0.8, r'$S(x) = \\frac{1}{1+e^{-x}}$', ha='center', va='center', size=16)\n",
    "plt.title('Sigmoid');\n",
    "fig.savefig('plots/sigmoid.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
